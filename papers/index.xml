<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Papers on NetKet</title><link>/papers/</link><description>Recent content in Papers on NetKet</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright 2019-2022, The Netket authors - All rights reserved.</copyright><lastBuildDate>Mon, 14 Feb 2022 11:12:50 +0100</lastBuildDate><atom:link href="/papers/index.xml" rel="self" type="application/rss+xml"/><item><title>Looking elsewhere: improving variational Monte Carlo gradients by importance sampling</title><link>/papers/misery-importance-sampling/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>/papers/misery-importance-sampling/</guid><description>The paper discusses improving neural-network quantum states (NQS) training via Variational Monte Carlo (VMC) methods. The authors propose a systematic strategy to tackle sampling issues by means of adaptively tuned importance sampling that can reduce the computational cost of vanilla VMC considerably, up to a factor of 100x when targeting highly peaked quantum chemistry wavefunctions.</description></item><item><title>Variational subspace methods and application to improving variational Monte Carlo dynamics</title><link>/papers/kahn-variational-subspace/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>/papers/kahn-variational-subspace/</guid><description>We present a formalism that allows for the direct manipulation and optimization of subspaces, circumventing the need to optimize individual states when using subspace methods. Using the determinant state mapping, we can naturally extend notions such as distance and energy to subspaces, as well as Monte Carlo estimators, recovering the excited states estimation method proposed by Pfau et al. As a practical application, we then introduce Bridge, a method that improves the performance of variational dynamics by extracting linear combinations of variational time-evolved states.</description></item><item><title>Neural Projected Quantum Dynamics: a systematic study</title><link>/papers/gravina-pvqd-systematic/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>/papers/gravina-pvqd-systematic/</guid><description>We address the challenge of simulating unitary quantum dynamics in large systems using Neural Quantum States, focusing on overcoming the computational instabilities and high cost of existing methods. This work offers a comprehensive formalization of the projected time-dependent Variational Monte Carlo (p-tVMC) method by thoroughly analyzing its two essential components: stochastic infidelity minimization and discretization of the unitary evolution. We investigate neural infidelity minimization using natural gradient descent strategies, identifying the most stable stochastic estimators and introducing adaptive regularization strategies that eliminate the need for manual adjustment of the hyperparameter along the dynamics.</description></item><item><title>Simple Fermionic backflow states via a systematically improvable tensor decomposition</title><link>/papers/bortone-gpsket-backflow/</link><pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate><guid>/papers/bortone-gpsket-backflow/</guid><description>We present an effective ansatz for the wave function of correlated electrons that brings closer the fields of machine learning parameterizations and tensor rank decompositions. We consider a CANDECOMP/PARAFAC (CP) tensor factorization of a general backflow transformation in second quantization for a simple, compact and systematically improvable Fermionic state. This directly encodes N-body correlations without the ordering dependence of other tensor decompositions. We consider and explicitly demonstrate various controllable truncations, in the rank and range of the backflow correlations or magnitude of local energy contributions, in order to systematically affect scaling reductions to O(N¬≥)-O(N‚Å¥).</description></item><item><title>Empirical sample complexity of neural network mixed state reconstruction</title><link>/papers/zhao-sample-complexity-ndm/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>/papers/zhao-sample-complexity-ndm/</guid><description>Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques.</description></item><item><title>Efficiency of neural quantum states in light of the quantum geometric tensor</title><link>/papers/dash-qgt/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>/papers/dash-qgt/</guid><description>Neural quantum state (NQS) ansaÃàtze have shown promise in variational Monte Carlo algorithms by their theoretical capability of representing any quantum state. However, the reason behind the practical improvement in their performance with an increase in the number of parameters is not fully understood. In this work, we systematically study the efficiency of restricted Boltzmann Machines (RBMs) to repre- sent the ground states in different phases of the spin-1 bilinear-biquadratic model, as the hidden layer density Œ± increases.</description></item><item><title>Impact of conditional modelling for a universal autoregressive quantum state</title><link>/papers/bortone-gpsket-autoreg/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>/papers/bortone-gpsket-autoreg/</guid><description>We present a generalized framework to adapt universal quantum state approximators, enabling them to satisfy rigorous normalization and autoregressive properties. We also introduce filters as analogues to convolutional layers in neural networks to incorporate translationally symmetrized correlations in arbitrary quantum states. By applying this framework to the Gaussian process state, we enforce autoregressive and/or filter properties, analyzing the impact of the resulting inductive biases on variational flexibility, symmetries, and conserved quantities.</description></item><item><title>A simple linear algebra identity to optimize large-scale neural network quantum states</title><link>/papers/rende-linearalgebrasimple/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>/papers/rende-linearalgebrasimple/</guid><description>Neural-network architectures have been increasingly used to represent quantum many-body wave functions. These networks require a large number of variational parameters and are challenging to optimize using traditional methods, as gradient descent. Stochastic reconfiguration (SR) has been effective with a limited number of parameters, but becomes impractical beyond a few thousand parameters. Here, we leverage a simple linear algebra identity to show that SR can be employed even in the deep learning scenario.</description></item><item><title>Variational Embeddings for Many Body Quantum Systems</title><link>/papers/barison-embedding/</link><pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate><guid>/papers/barison-embedding/</guid><description>We propose a variational scheme to represent composite quantum systems using multiple param- eterized functions of varying accuracies on both classical and quantum hardware. The approach follows the variational principle over the entire system, and is naturally suited for scenarios where an accurate description is only needed in a smaller subspace. We show how to include quantum devices as high-accuracy solvers on these correlated degrees of freedom, while handling the remain- ing contributions with a classical device.</description></item><item><title>Unbiasing time-dependent Variational Monte Carlo by projected quantum evolution</title><link>/papers/sinibaldi-unbiasing/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>/papers/sinibaldi-unbiasing/</guid><description>We analyze the accuracy and sample complexity of variational Monte Carlo approaches to simulate the dynamics of many-body quantum systems classically. By systematically studying the relevant stochastic estimators, we are able to: (i) prove that the most used scheme, the time-dependent Variational Monte Carlo (tVMC), is affected by a systematic statistical bias or exponential sample complexity when the wave function contains some (possibly approximate) zeros, an important case for fermionic systems and quantum information protocols; (ii) show that a different scheme based on the solution of an optimization problem at each time step is free from such problems; (iii) improve the sample complexity of this latter approach by several orders of magnitude with respect to previous proofs of concept.</description></item><item><title>High-accuracy variational Monte Carlo for frustrated magnets with deep neural networks</title><link>/papers/roth-highaccuracyfrustratedmagnets/</link><pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate><guid>/papers/roth-highaccuracyfrustratedmagnets/</guid><description>We show that neural quantum states based on very deep (4‚Äì16-layered) neural networks can outperform state-of-the-art variational approaches on highly frustrated quantum magnets, including quantum-spin-liquid candidates. We focus on group convolutional neural networks that allow us to efficiently impose space-group symmetries on our ans√§tze. We achieve state-of-the-art ground-state energies for the ùêΩ1‚àíùêΩ2 Heisenberg models on the square and triangular lattices, in both ordered and spin-liquid phases, and discuss ways to access low-lying excited states in nontrivial symmetry sectors.</description></item><item><title>NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems</title><link>/papers/vicentini-netket3/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>/papers/vicentini-netket3/</guid><description>We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans√§tze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation.</description></item><item><title>Positive-definite parametrization of mixed quantum states with deep neural networks</title><link>/papers/vicentini-positive-def-ndm/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>/papers/vicentini-positive-def-ndm/</guid><description>We introduce the Gram-Hadamard Density Operator (GHDO), a new deep neural-network architecture that can encode positive semi-definite density operators of exponential rank with polynomial resources. We then show how to embed an autoregressive structure in the GHDO to allow direct sampling of the probability distribution. These properties are especially important when representing and variationally optimizing the mixed quantum state of a system interacting with an environment. Finally, we benchmark this architecture by simulating the steady state of the dissipative transverse-field Ising model.</description></item></channel></rss>