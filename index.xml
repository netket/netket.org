<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on NetKet</title><link>/</link><description>Recent content in Home on NetKet</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright 2019-2022, The Netket authors - All rights reserved.</copyright><lastBuildDate>Tue, 01 Feb 2022 17:48:04 +0100</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Get Involved</title><link>/get_involved/</link><pubDate>Tue, 01 Feb 2022 18:02:27 +0100</pubDate><guid>/get_involved/</guid><description>Getting In Touch with the community Users and developers of NetKet hang out in the following places:
GitHub Discussions: we encourage users who have questions about NetKet and Neural Quantum States in general to start a discussion thread. We like them because they are not ephemeral, they are indexed by search engines, and if you get an answer it might benefit other people in the future. GitHub Issues: if you believe you have found a bug, or have a feature request or just want to discuss something to implement in NetKet, we encourage you to start an Issue on GitHub!</description></item><item><title>Citing</title><link>/cite/</link><pubDate>Tue, 01 Feb 2022 18:02:27 +0100</pubDate><guid>/cite/</guid><description>If you find the NetKet project useful for your research, please consider citing our relevant software papers in order to help us to continue devoting time and resources to NetKet development.
The current version of NetKet 3.x is described in our recent preprint arXiv:2112.10526.
NetKet 2.x is described in SoftwareX 10, 100311 (2019). Despite many new features, NetKet 3 still builds on the foundations of the NetKet 2 era too. Therefore, we would appreciate it if you cite both papers in order to continue giving credit to all the authors of NetKet 2 as well.</description></item><item><title>Simple Fermionic backflow states via a systematically improvable tensor decomposition</title><link>/papers/bortone-gpsket-backflow/</link><pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate><guid>/papers/bortone-gpsket-backflow/</guid><description>We present an effective ansatz for the wave function of correlated electrons that brings closer the fields of machine learning parameterizations and tensor rank decompositions. We consider a CANDECOMP/PARAFAC (CP) tensor factorization of a general backflow transformation in second quantization for a simple, compact and systematically improvable Fermionic state. This directly encodes N-body correlations without the ordering dependence of other tensor decompositions. We consider and explicitly demonstrate various controllable truncations, in the rank and range of the backflow correlations or magnitude of local energy contributions, in order to systematically affect scaling reductions to O(N¬≥)-O(N‚Å¥).</description></item><item><title>Empirical sample complexity of neural network mixed state reconstruction</title><link>/papers/zhao-sample-complexity-ndm/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>/papers/zhao-sample-complexity-ndm/</guid><description>Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques.</description></item><item><title>Efficiency of neural quantum states in light of the quantum geometric tensor</title><link>/papers/dash-qgt/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>/papers/dash-qgt/</guid><description>Neural quantum state (NQS) ansaÃàtze have shown promise in variational Monte Carlo algorithms by their theoretical capability of representing any quantum state. However, the reason behind the practical improvement in their performance with an increase in the number of parameters is not fully understood. In this work, we systematically study the efficiency of restricted Boltzmann Machines (RBMs) to repre- sent the ground states in different phases of the spin-1 bilinear-biquadratic model, as the hidden layer density Œ± increases.</description></item><item><title>Impact of conditional modelling for a universal autoregressive quantum state</title><link>/papers/bortone-gpsket-autoreg/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>/papers/bortone-gpsket-autoreg/</guid><description>We present a generalized framework to adapt universal quantum state approximators, enabling them to satisfy rigorous normalization and autoregressive properties. We also introduce filters as analogues to convolutional layers in neural networks to incorporate translationally symmetrized correlations in arbitrary quantum states. By applying this framework to the Gaussian process state, we enforce autoregressive and/or filter properties, analyzing the impact of the resulting inductive biases on variational flexibility, symmetries, and conserved quantities.</description></item><item><title>A simple linear algebra identity to optimize large-scale neural network quantum states</title><link>/papers/rende-linearalgebrasimple/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>/papers/rende-linearalgebrasimple/</guid><description>Neural-network architectures have been increasingly used to represent quantum many-body wave functions. These networks require a large number of variational parameters and are challenging to optimize using traditional methods, as gradient descent. Stochastic reconfiguration (SR) has been effective with a limited number of parameters, but becomes impractical beyond a few thousand parameters. Here, we leverage a simple linear algebra identity to show that SR can be employed even in the deep learning scenario.</description></item><item><title>Variational Embeddings for Many Body Quantum Systems</title><link>/papers/barison-embedding/</link><pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate><guid>/papers/barison-embedding/</guid><description>We propose a variational scheme to represent composite quantum systems using multiple param- eterized functions of varying accuracies on both classical and quantum hardware. The approach follows the variational principle over the entire system, and is naturally suited for scenarios where an accurate description is only needed in a smaller subspace. We show how to include quantum devices as high-accuracy solvers on these correlated degrees of freedom, while handling the remain- ing contributions with a classical device.</description></item><item><title>Unbiasing time-dependent Variational Monte Carlo by projected quantum evolution</title><link>/papers/sinibaldi-unbiasing/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>/papers/sinibaldi-unbiasing/</guid><description>We analyze the accuracy and sample complexity of variational Monte Carlo approaches to simulate the dynamics of many-body quantum systems classically. By systematically studying the relevant stochastic estimators, we are able to: (i) prove that the most used scheme, the time-dependent Variational Monte Carlo (tVMC), is affected by a systematic statistical bias or exponential sample complexity when the wave function contains some (possibly approximate) zeros, an important case for fermionic systems and quantum information protocols; (ii) show that a different scheme based on the solution of an optimization problem at each time step is free from such problems; (iii) improve the sample complexity of this latter approach by several orders of magnitude with respect to previous proofs of concept.</description></item><item><title>High-accuracy variational Monte Carlo for frustrated magnets with deep neural networks</title><link>/papers/roth-highaccuracyfrustratedmagnets/</link><pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate><guid>/papers/roth-highaccuracyfrustratedmagnets/</guid><description>We show that neural quantum states based on very deep (4‚Äì16-layered) neural networks can outperform state-of-the-art variational approaches on highly frustrated quantum magnets, including quantum-spin-liquid candidates. We focus on group convolutional neural networks that allow us to efficiently impose space-group symmetries on our ans√§tze. We achieve state-of-the-art ground-state energies for the ùêΩ1‚àíùêΩ2 Heisenberg models on the square and triangular lattices, in both ordered and spin-liquid phases, and discuss ways to access low-lying excited states in nontrivial symmetry sectors.</description></item><item><title>NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems</title><link>/papers/vicentini-netket3/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>/papers/vicentini-netket3/</guid><description>We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans√§tze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation.</description></item><item><title>Positive-definite parametrization of mixed quantum states with deep neural networks</title><link>/papers/vicentini-positive-def-ndm/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>/papers/vicentini-positive-def-ndm/</guid><description>We introduce the Gram-Hadamard Density Operator (GHDO), a new deep neural-network architecture that can encode positive semi-definite density operators of exponential rank with polynomial resources. We then show how to embed an autoregressive structure in the GHDO to allow direct sampling of the probability distribution. These properties are especially important when representing and variationally optimizing the mixed quantum state of a system interacting with an environment. Finally, we benchmark this architecture by simulating the steady state of the dissipative transverse-field Ising model.</description></item><item><title>UnitaryHack: Win bounties with NetKet</title><link>/posts/3-unitaryhack/</link><pubDate>Sun, 01 May 2022 10:23:16 +0100</pubDate><guid>/posts/3-unitaryhack/</guid><description>NetKet is participating in the 2022 edition of UnitaryHack, kindly sponsored by the quantum non-profit Unitary Fund.
The hackathon will run for two weeks, from Friday, June 3 to Friday, June 17. During the hackathon, several high-profile quantum software projects are each proposing several tasks for which Unitary Fund will provide a bounty to the participant completing the task.
NetKet is participating with 4 bounties to this hackathon, which are worth US$ 300 in total.</description></item><item><title>NetKet-friendly Summer School in Toulouse</title><link>/posts/02-toulouse/</link><pubDate>Mon, 14 Feb 2022 10:23:16 +0100</pubDate><guid>/posts/02-toulouse/</guid><description>We are glad to advertise that our dear colleagues in Toulouse are organising a Summer school in April on Machine Learning techniques for Quantum Many-Body Physics. The school will feature several lectures and tutorials on NetKet.
Below you can find the original announcement.
Announcement Toulouse School on Machine Learning for Quantum Many-Body Physics
that will take place in Toulouse, France (4th-8th April 2022).
The school will take place in hybrid format (with both online and onsite students and lecturers).</description></item><item><title>Get Started</title><link>/get_started/</link><pubDate>Tue, 01 Feb 2022 18:02:27 +0100</pubDate><guid>/get_started/</guid><description>QuickStart on the Cloud The simplest way to get started with NetKet is to use Google&amp;rsquo;s Colab, an online notebook environment with access to GPUs. If you have a google, account, simply head to NetKet&amp;rsquo;s documentation, select the first tutorial, and click on the small rocket icon on the top navigation bar. This will launch the tutorial on an online environment that is ready to use.
For example, you can simply click on this button and get started!</description></item><item><title>NetKet 3 ‚ù§Ô∏è Jax</title><link>/posts/01-welcome/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>/posts/01-welcome/</guid><description>18 months in development, NetKet 3.0 indicates a major new step for the NetKet project. NetKet has been totally rewritten in Python and is now a Jax-based library. This guarantees outstanding performance while allowing researchers to exploit machine-learning frameworks to define advanced Neural-Networks.
GPUs and Google‚Äôs TPUs are now supported too!
Update now and try the new examples!</description></item></channel></rss>